{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9ead2e",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE) for Financial Anomaly Detection\n",
    "\n",
    "This report presents an anomaly detection system for financial time series data using a Variational Autoencoder (VAE) with LSTM architecture. The model is trained on daily trading data from the CPALL stock and evaluated for its ability to identify anomalous trading patterns.\n",
    "\n",
    "## Objectives\n",
    "- Process and analyze financial trading data\n",
    "- Develop a VAE-LSTM model for anomaly detection\n",
    "- Evaluate model performance for identifying market anomalies\n",
    "- Visualize results with anomaly indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f06a38",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Environment Setup\n",
    "\n",
    "First, we set up the environment and load the required data. We're using CPALL stock data from a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb33908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_root import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Setup project directories\n",
    "project_root = Path(PROJECT_DIR)\n",
    "data_dir = project_root / \"data\"\n",
    "\n",
    "# Ensure local data directory exists\n",
    "local_data_dir = Path(\"./data\")\n",
    "local_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "tickmatch_dir = data_dir / \"tickmatch\"\n",
    "bidask_dir = data_dir / \"bidask\"\n",
    "\n",
    "tm_intra_dir = tickmatch_dir / \"intraday\"\n",
    "ba_intra_dir = bidask_dir / \"intraday\"\n",
    "\n",
    "print(f\"Data directories initialized. Local data will be saved to {local_data_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce179db",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "We import the necessary libraries for data processing, model building, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4bd00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb523dc",
   "metadata": {},
   "source": [
    "## 3. Load and Process Trading Data\n",
    "\n",
    "We load trading data for CPALL stock and process it to create daily aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872eab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"CPALL\"\n",
    "\n",
    "# Load trade data from parquet file\n",
    "tm_symbol_dir = tm_intra_dir / f\"{symbol}.parquet\"\n",
    "tm_df = pd.read_parquet(tm_symbol_dir)\n",
    "\n",
    "print(f\"Loaded {len(tm_df):,} records for {symbol}\")\n",
    "tm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe2f69",
   "metadata": {},
   "source": [
    "### 3.1 Extract Daily Price Series\n",
    "\n",
    "We extract the daily closing prices for later analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract daily last prices\n",
    "daily_last = tm_df.set_index(\"time\")[\"last\"].resample(\"1D\").last().dropna()\n",
    "\n",
    "print(f\"Daily price series spans from {daily_last.index.min().date()} to {daily_last.index.max().date()}\")\n",
    "daily_last.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e76148",
   "metadata": {},
   "source": [
    "### 3.2 Create Daily Trading Aggregations\n",
    "\n",
    "We aggregate the tick data into daily metrics including volume, trade count, buy/sell volume, and average price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d6aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate buy/sell volumes by day\n",
    "vol_df = tm_df.pivot_table(index='time', values='vol', columns='type')[['BUY', 'SELL']].\\\n",
    "        rename(columns={'BUY': 'buy_vol', 'SELL': 'sell_vol'}).fillna(0).\\\n",
    "        groupby([pd.Grouper(freq='D')]).agg({'buy_vol': 'sum', 'sell_vol': 'sum'})\n",
    "\n",
    "# Create daily aggregations of price and volume data\n",
    "tm_daily = tm_df.groupby([pd.Grouper(key=\"time\", freq=\"D\")]).agg({\n",
    "            'last': 'mean',\n",
    "            'vol':'sum',\n",
    "            'id':'count',\n",
    "        }).\\\n",
    "        dropna().\\\n",
    "        rename({\n",
    "            'last':'price_avg',\n",
    "            'vol':'vol_sum',\n",
    "            'id':'id_count'\n",
    "        }, axis=1)\n",
    "\n",
    "# Merge daily aggregations with buy/sell volumes\n",
    "tm_daily = tm_daily.merge(vol_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Save the daily data for future use\n",
    "tm_daily.to_csv(local_data_dir / f\"{symbol}_daily.csv\")\n",
    "print(f\"Saved daily aggregations to {local_data_dir / f'{symbol}_daily.csv'}\")\n",
    "\n",
    "tm_daily.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34803a17",
   "metadata": {},
   "source": [
    "## 4. Data Splitting and Preparation for Modeling\n",
    "\n",
    "We split the data into training and testing sets for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c18de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and testing (30%) sets\n",
    "n_days = len(tm_daily)\n",
    "train_days = int(n_days * 0.7)\n",
    "tm_daily_train = tm_daily.iloc[:train_days]\n",
    "tm_daily_test = tm_daily.iloc[train_days:]\n",
    "\n",
    "print(f\"Total trading days: {n_days}\")\n",
    "print(f\"Training set: {len(tm_daily_train)} days ({train_days/n_days:.1%})\")\n",
    "print(f\"Test set: {len(tm_daily_test)} days ({len(tm_daily_test)/n_days:.1%})\")\n",
    "\n",
    "print(\"\\nTraining period:\")\n",
    "print(f\"From: {tm_daily_train.index.min().date()} to {tm_daily_train.index.max().date()}\")\n",
    "\n",
    "print(\"\\nTest period:\")\n",
    "print(f\"From: {tm_daily_test.index.min().date()} to {tm_daily_test.index.max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458a6f4",
   "metadata": {},
   "source": [
    "### 4.1 Sequence Generation for Time Series Modeling\n",
    "\n",
    "We create sequences of previous days' data to use as input for our LSTM-VAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequence length (number of previous days to use for prediction)\n",
    "seq_len = 20\n",
    "print(f\"Using {seq_len} previous trading days to create each input sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95224c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences from time series data\n",
    "def create_sequences(data, seq_len=20):\n",
    "    \"\"\"\n",
    "    Create sequences from time series data with specified sequence length.\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame): Time series data\n",
    "        seq_len (int): Length of sequence to generate\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of sequences\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_len + 1):\n",
    "        seq = data[i:i+seq_len]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Standardize the data for better model performance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create training sequences\n",
    "X_train = create_sequences(tm_daily_train, seq_len=seq_len)\n",
    "X_val = create_sequences(tm_daily_train[1:], seq_len=seq_len)  # Shifted by 1 day for validation\n",
    "\n",
    "# Apply standardization\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "\n",
    "print(f\"Training sequences shape: {X_train.shape}\")\n",
    "print(f\"Validation sequences shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6580121",
   "metadata": {},
   "source": [
    "## 5. LSTM-VAE Model Architecture\n",
    "\n",
    "We define the LSTM-VAE architecture for anomaly detection in time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "class VAE(models.Model):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder model for time series anomaly detection\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get latent space representation\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        \n",
    "        # Focus on the last timestep for reconstruction loss\n",
    "        x_true_last = inputs[:, -1:, :]  # shape: (batch_size, 1, num_features)\n",
    "\n",
    "        # Calculate reconstruction loss for the last timestep\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(x_true_last - reconstructed))\n",
    "\n",
    "        # Calculate KL divergence loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "        )\n",
    "\n",
    "        # Total loss = reconstruction loss + KL loss\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        self.add_loss(total_loss)\n",
    "        \n",
    "        return reconstructed\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"\n",
    "    Sampling layer for VAE - samples from the latent distribution\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ae5ac",
   "metadata": {},
   "source": [
    "### 5.1 Model Building Function\n",
    "\n",
    "We define a function to build the LSTM-VAE model with encoder and decoder components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_vae(sequence_length, num_features, latent_dim):\n",
    "    \"\"\"\n",
    "    Build LSTM-VAE model for time series anomaly detection\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: Length of input time series sequence\n",
    "        num_features: Number of features in each timestep\n",
    "        latent_dim: Dimension of the latent space\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vae_model, encoder_model, decoder_model)\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    encoder_inputs = layers.Input(shape=(sequence_length, num_features))\n",
    "    lstm_enc = layers.LSTM(64, return_sequences=True, activation='relu')(encoder_inputs)\n",
    "    lstm_enc = layers.BatchNormalization()(lstm_enc)  # Batch normalization for better training\n",
    "    lstm_enc = layers.LSTM(32, return_sequences=False, activation='relu')(lstm_enc)\n",
    "    lstm_enc = layers.BatchNormalization()(lstm_enc)  # Batch normalization\n",
    "    \n",
    "    # Latent space parameters\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(lstm_enc)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(lstm_enc)\n",
    "    z = Sampling()([z_mean, z_log_var])  # Sample from latent distribution\n",
    "\n",
    "    encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "    z_reshaped = layers.Reshape((1, latent_dim))(decoder_inputs)\n",
    "    decoder_lstm = layers.LSTM(32, return_sequences=True, activation='relu')(z_reshaped)\n",
    "    decoder_lstm = layers.BatchNormalization()(decoder_lstm)  # Batch normalization\n",
    "    decoder_lstm = layers.LSTM(64, return_sequences=True, activation='relu')(decoder_lstm)\n",
    "    decoder_lstm = layers.BatchNormalization()(decoder_lstm)  # Batch normalization\n",
    "    \n",
    "    decoder_outputs = layers.Dense(num_features)(decoder_lstm)\n",
    "\n",
    "    decoder = models.Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    # Combine encoder and decoder into VAE\n",
    "    vae = VAE(encoder, decoder)\n",
    "    return vae, encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d711908",
   "metadata": {},
   "source": [
    "### 5.2 Model Training\n",
    "\n",
    "We build and train the LSTM-VAE model on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the VAE model\n",
    "latent_dim = 12  # Dimension of latent space\n",
    "vae, encoder, decoder = build_lstm_vae(\n",
    "    sequence_length=seq_len, \n",
    "    num_features=X_train.shape[2], \n",
    "    latent_dim=latent_dim\n",
    ")\n",
    "\n",
    "# Configure optimizer and compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01) \n",
    "vae.compile(optimizer=optimizer)\n",
    "\n",
    "# Display model summary\n",
    "print(\"Encoder Summary:\")\n",
    "encoder.summary()\n",
    "print(\"\\nDecoder Summary:\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = vae.fit(\n",
    "    X_train, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_val, None),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "vae.save(local_data_dir / f\"{symbol}_vae_model\")\n",
    "print(f\"Model saved to {local_data_dir / f'{symbol}_vae_model'}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.title('Model Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc365c",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection on Test Data\n",
    "\n",
    "We evaluate the model on test data to detect anomalous patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences from test data\n",
    "X_test_seq = create_sequences(tm_daily_test, seq_len=seq_len)\n",
    "\n",
    "# Standardize test data using the same scaler\n",
    "X_test_seq_scaled = scaler.transform(X_test_seq.reshape(-1, X_test_seq.shape[-1])).reshape(X_test_seq.shape)\n",
    "\n",
    "print(f\"Test sequences shape: {X_test_seq_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09531c17",
   "metadata": {},
   "source": [
    "### 6.1 Calculating Reconstruction Loss\n",
    "\n",
    "We calculate reconstruction loss for each test sequence to identify anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36256b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction loss for each test sequence\n",
    "all_loss = []\n",
    "for x_test in tqdm(X_test_seq_scaled, desc=\"Evaluating test sequences\"):\n",
    "    x_test = x_test.reshape(1, x_test.shape[0], x_test.shape[1])  # Reshape for model input\n",
    "    loss = vae.evaluate(x_test, x_test, verbose=0)  # Calculate reconstruction loss\n",
    "    all_loss.append(loss)\n",
    "\n",
    "all_loss = np.array(all_loss)\n",
    "\n",
    "# Save loss values\n",
    "loss_df = pd.DataFrame(all_loss, index=tm_daily_test.index[-len(all_loss):], columns=['loss'])\n",
    "loss_df.to_csv(local_data_dir / f\"{symbol}_anomaly_scores.csv\")\n",
    "print(f\"Anomaly scores saved to {local_data_dir / f'{symbol}_anomaly_scores.csv'}\")\n",
    "\n",
    "print(f\"Min loss: {all_loss.min():.4f}, Max loss: {all_loss.max():.4f}, Mean loss: {all_loss.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29218ab7",
   "metadata": {},
   "source": [
    "### 6.2 Visualizing the Distribution of Anomaly Scores\n",
    "\n",
    "We plot the distribution of reconstruction losses to understand what constitutes an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f642431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(all_loss, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Reconstruction Loss (Anomaly Score)', fontsize=15)\n",
    "plt.xlabel('Reconstruction Loss', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Add threshold line for anomaly detection\n",
    "threshold = all_loss.mean() + all_loss.std()\n",
    "plt.axvline(x=threshold, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Anomaly Threshold: {threshold:.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(local_data_dir / f\"{symbol}_loss_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a42b842",
   "metadata": {},
   "source": [
    "### 6.3 Preparing Data for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the price series for the same period as our anomaly scores\n",
    "last_plot = daily_last.iloc[-len(all_loss):].copy()\n",
    "last_plot.index = pd.to_datetime(last_plot.index)\n",
    "\n",
    "# Create a DataFrame with price and anomaly scores\n",
    "last_loss = pd.DataFrame(all_loss, index=last_plot.index, columns=['loss'])\n",
    "last_loss['last'] = last_plot.values\n",
    "\n",
    "print(\"Combined data for visualization:\")\n",
    "last_loss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac376f81",
   "metadata": {},
   "source": [
    "## 7. Visualization of Results\n",
    "\n",
    "We visualize the anomaly scores alongside the price chart to identify unusual market behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define anomaly threshold\n",
    "anomaly_threshold = all_loss.mean() + 0.5 * all_loss.std()\n",
    "\n",
    "# Create the visualization\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot loss values\n",
    "ax.plot(last_loss.index, last_loss['loss'], label='Reconstruction Loss', color='blue', ls='--')\n",
    "ax.set_title('VAE Reconstruction Loss and Stock Price', fontsize=16)\n",
    "ax.set_ylabel('Reconstruction Loss', fontsize=14)\n",
    "ax.set_xlabel('Date', fontsize=14)\n",
    "ax.axhline(y=anomaly_threshold, color='purple', linestyle='-', alpha=0.7, \n",
    "           label=f'Anomaly Threshold: {anomaly_threshold:.2f}')\n",
    "\n",
    "# Create second y-axis for price\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(last_plot, label='Stock Price', color='red')\n",
    "ax2.set_ylabel('Price (THB)', fontsize=14)\n",
    "\n",
    "# Highlight anomalies\n",
    "ax2.fill_between(\n",
    "    last_loss.index,\n",
    "    last_loss['last'].min(), \n",
    "    last_loss['last'], \n",
    "    alpha=0.5, \n",
    "    color='red', \n",
    "    where=last_loss['loss'] > anomaly_threshold, \n",
    "    label='Anomaly'\n",
    ")\n",
    "\n",
    "# Create a combined legend for both axes\n",
    "lines1, labels1 = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(lines1 + lines2, labels1 + labels2, loc='best', fontsize=12)\n",
    "\n",
    "# Format the grid\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the visualization\n",
    "plt.savefig(local_data_dir / f\"{symbol}_anomaly_detection.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb65f2",
   "metadata": {},
   "source": [
    "## 8. Quantitative Analysis of Anomalies\n",
    "\n",
    "We analyze the detected anomalies and their relationship with price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83830eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify anomalies based on threshold\n",
    "anomalies = last_loss[last_loss['loss'] > anomaly_threshold].copy()\n",
    "print(f\"Detected {len(anomalies)} anomalies out of {len(last_loss)} trading days ({len(anomalies)/len(last_loss):.1%})\")\n",
    "\n",
    "# Calculate price changes during anomalies\n",
    "last_loss['price_change'] = last_loss['last'].pct_change() * 100\n",
    "anomalies['price_change'] = anomalies['last'].pct_change() * 100\n",
    "\n",
    "# Compare price volatility\n",
    "normal_volatility = last_loss[last_loss['loss'] <= anomaly_threshold]['price_change'].std()\n",
    "anomaly_volatility = anomalies['price_change'].std()\n",
    "\n",
    "print(f\"\\nPrice change statistics:\")\n",
    "print(f\"Normal days volatility: {normal_volatility:.2f}%\")\n",
    "print(f\"Anomaly days volatility: {anomaly_volatility:.2f}%\")\n",
    "print(f\"Ratio: {anomaly_volatility/normal_volatility:.2f}x higher during anomalies\")\n",
    "\n",
    "# Show the top 10 anomalies\n",
    "print(\"\\nTop 10 anomalies by reconstruction loss:\")\n",
    "top_anomalies = last_loss.sort_values('loss', ascending=False).head(10)\n",
    "top_anomalies[['loss', 'last', 'price_change']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa117bf7",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Future Work\n",
    "\n",
    "Our LSTM-VAE model has successfully identified anomalous trading patterns in CPALL stock data. The anomaly detection system shows potential for identifying unusual market behavior that might indicate significant market events or information asymmetry.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. The model effectively identifies days with abnormal trading patterns based on volume and price features\n",
    "2. Anomalous trading days often coincide with higher price volatility\n",
    "3. The VAE reconstruction loss provides a continuous measure of abnormality, allowing for threshold adjustment\n",
    "\n",
    "### Future Work:\n",
    "\n",
    "1. Incorporate additional features such as market indices and sector performance\n",
    "2. Test the model on different time frames (hourly, weekly) for multi-scale anomaly detection\n",
    "3. Develop an early warning system by predicting future anomalies\n",
    "4. Optimize hyperparameters for improved anomaly detection accuracy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
